{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "C9 Forecasting.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nrkfeller/YCBS_notes/blob/master/C9_Forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMLXsS_patjW",
        "colab_type": "text"
      },
      "source": [
        "# Time Series Problem\n",
        "We create a univariate time series. As opposed to multivariate time series which is like financial information over time like: price, volume, revenues, debt, etc.\n",
        "\n",
        "In this case we will try to predict future values, but it could be an imputation problem where we attempt to fill in missing values. This is one of the early uses of neural networks. Where scientists used RNNs to fill missing values in databases.\n",
        "\n",
        "### Generate Time Series\n",
        "* This functino generates as many times series as requested by the ```batch_size```\n",
        "* Each time series is of length ```n_steps```\n",
        "* it creates a series made of two sine waves with random frequencies and phases with a bit of noise. The amplitudes are fixed\n",
        "* returns a numpy array of ```[batch_size, n_steps, 1]```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4UJ8AWj-wj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWaaZTJhBTcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj3ScRwZ4ixl",
        "colab_type": "text"
      },
      "source": [
        "### Creating Data Splits\n",
        "1. X_train is 7000 time series of 50 values\n",
        "2. X_valid is 2000 times series of 50 values\n",
        "3. All Y values are groups of single values that come imediately after the X times series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPX-SGirBZxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n",
        "\n",
        "for i in range(1):\n",
        "  plt.plot(series[i].reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFcE94rODXSE",
        "colab_type": "text"
      },
      "source": [
        "## Baseline Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSUX76YtBgSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_pred = X_valid[:, -1]\n",
        "\n",
        "# our baseline error is about 0.02, se we must do better\n",
        "mean_squared_error(y_pred, y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoyUGx61CHcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_linear = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[50,1]),\n",
        "    keras.layers.Dense(1, input_shape=[50,1])\n",
        "])\n",
        "\n",
        "model_linear.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "model_linear.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQxD6_PqClwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_linear.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkaWNcXrDOix",
        "colab_type": "text"
      },
      "source": [
        "## Implementing a Simple RNN\n",
        "* We create a single layer, single neuron RNN ```SimpleRNN```\n",
        "* The default activation is ```tanh```\n",
        "* In this simple RNN, the output is the same as the state. Yt = Ht\n",
        "* The default output for ```SimpleRNN``` is the output at the last timestep. In our example the output is Y at t=49\n",
        "* If we want the output all all timesteps we must set ```return_sequences=True```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZExvxp73D-Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "?keras.layers.SimpleRNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5eC3mUkC0ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rec = keras.models.Sequential([\n",
        "    # We don't specify the length of the input sequence since RNNs can process any number of timesteps\n",
        "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
        "])\n",
        "\n",
        "model_rec.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "model_rec.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), verbose=0)\n",
        "\n",
        "model_rec.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsSyPUjaEo-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_linear.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7jizhuce2OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# our RNN model doesn't perform as well, but it has much less parameters\n",
        "model_rec.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soSM4m41fbZf",
        "colab_type": "text"
      },
      "source": [
        "## Deep RNNs\n",
        "* We must set ```return_sequences=True``` so that we can propagate backwards the correct size. This means that we propagate a sequence of errors instead of just the last errors\n",
        "* We are still using a single output at the last layer. This means that the hidden state for this layer is just one unit.\n",
        "* We can also replace the last layer by a Dense layer, it wouldn't change the performance and it would make the training faster. We just need to remove the ```return_sequence=True``` from the second layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd4UswIDe7xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try a more complex network\n",
        "\n",
        "model_rec_deep = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model_rec_deep.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "model_rec_deep.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIuPzjREgChL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_rec_deep.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP27RnUmh6wv",
        "colab_type": "text"
      },
      "source": [
        "## Forecasting Several Time Steps Ahead\n",
        "Lets try to predict 10 steps ahead and the next 10 values\n",
        "\n",
        "1. The first option is to use the model we already trained, make it predict the next value, then add that value to the inputs (acting as if this predicted value had actually occurred), and use the model again to predict the following value, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8Ig7HKgl8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
        "X = X_new\n",
        "\n",
        "for step_ahead in range(10):\n",
        "  y_pred_one = model_rec_deep.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "  X = np.concatenate([X, y_pred_one], axis=1)\n",
        "  \n",
        "Y_pred = X[:, n_steps:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoVvKMHze9u1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(X.reshape(-1))\n",
        "plt.plot(np.linspace(50,60,10),Y_pred.reshape(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucR_bXd7tiQ4",
        "colab_type": "text"
      },
      "source": [
        "Instead of predicting the next value and adding that value to the sequence and predicting the next one and so on. We can just create a model that predicts 10 values at a time.\n",
        "\n",
        "For this we need the y's to be chunks of 10 subsequent values\n",
        "\n",
        "This is a sequence to vector RNN. Because it takes in a sequence and outputs a vector, in this case of 10 values.\n",
        "\n",
        "So simply put, at the last timestep we predict 10 values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G7ylFBns5_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "series = generate_time_series(10000, n_steps + 10)\n",
        "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
        "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
        "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1a3OLVTt5Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ten = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20),\n",
        "    keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model_ten.compile(loss='mse', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLYl1RP5t95n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ten.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRrr7tl7uDeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred = model_ten.predict(X_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe-BqozTueXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzRj4aXhTjSQ",
        "colab_type": "text"
      },
      "source": [
        "### Seq to seq\n",
        "In the above model we predict the next 10 values only at the very last time step. Let's make a model that predicts the next 10 values at each time step.\n",
        "\n",
        "We will get more error gradient and our model will be able to learn much more complex patterns.\n",
        "\n",
        "This will actually be faster and more robust!\n",
        "\n",
        "#### The model\n",
        "1. We need to set ```return_sequences=True``` for a seq2seq\n",
        "2. The ```TimeDistributed``` layer wraps any layer and applies it to every timestep. So it reshapes the inputs from ```[batch_size, n_steps, in_dims]``` to ```[batch_size, n_steps, out_dims]```. In this case we have a dense layer of size 10 per input, of which we have 20. The output here will not actually be a simple vector of size 10, but 10 independently generated values by 20 different dense layers that form a sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Y5rJ3-5GKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10-D vectors\n",
        "for step_ahead in range(1, 10 + 1):\n",
        "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
        "Y_train = Y[:7000]\n",
        "Y_valid = Y[7000:9000]\n",
        "Y_test = Y[9000:]\n",
        "\n",
        "Y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2PvphOC5LwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_seq2seq = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt2L80FNU2K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_seq2seq.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPgAAl54XDJ0",
        "colab_type": "text"
      },
      "source": [
        "We create a custom loss function that considers the MSE between all predictions and all true values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEouYOpVUbIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def last_time_step_mse(Y_true, Y_pred):\n",
        "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
        "\n",
        "model_seq2seq.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS33J1edWaYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_seq2seq.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cvvC5N6X3YG",
        "colab_type": "text"
      },
      "source": [
        "# Handling Long Sequences\n",
        "As we know RNNs suffer from vanishing gradient. It is also very hard to remember long term patterns as it is so eloquently stated by Bengio et al. in this paper: http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf\n",
        "\n",
        "### Fighting the Vanishing and Exploding Gradient Problem\n",
        "Exploding gradient is cause by what?\n",
        "\n",
        "How do we typically deal with exploding gradient:\n",
        "1. ReLu\n",
        "2. Good parameter initialization\n",
        "3. Faster optimization\n",
        "4. Dropout\n",
        "5. Batch Normalization\n",
        "\n",
        "ReLU doesn't work in this case and is even likely to make the gradients explode. So in RNNs the default is a saturating activation fucntion namely \"tanh\". If your training becomes unstable (which you can monitor with tensorboard) you can use gradient clipping.\n",
        "\n",
        "Batch Normalization has been shower not to work very well on RNNs. It seems that applying the same scale and offset to the inputs at each time step does not yeild good results.\n",
        "\n",
        "Layer Normalization is better than BN when working with RNNs. It works in a similar way, but rather thjan normalizing accross the batch dimension, it normalizes across the feature dimension. So normalization happens on a single examples features on all summed inputs. This works for time series because the values are typically all of similar nature. This can also have the same behavior in training as in testing, because it doesn't require a batch.\n",
        "\n",
        "### Implementation of Layer Norm Cell\n",
        "1. Inherit from the ```keras.layers.Layer``` class. We create ```state_size``` and ```output_size``` which are the same.\n",
        "2. We take a simple RNN cell, but with no activation, as we want the activation to take place after the layer norm\n",
        "3. We provide layer norm and activation\n",
        "4. ```call()``` takes two arguments, the ```inputs``` and the current time step and hidden ```states``` from the previous step\n",
        "5. Returns the outputs and the current state. Both of which are normalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Nwa_AqXt2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LNSimpleRNNCell(keras.layers.Layer):\n",
        "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.state_size = units\n",
        "        self.output_size = units\n",
        "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
        "                                                          activation=None)\n",
        "        self.layer_norm = keras.layers.LayerNormalization()\n",
        "        self.activation = keras.activations.get(activation)\n",
        "    def call(self, inputs, states):\n",
        "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
        "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
        "        return norm_outputs, [norm_outputs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op47SmECw_uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUl9cTK7DEwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you can add dropout as a hyperparameter of any RNN layers in keras\n",
        "?keras.layers.SimpleRNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu-o_9CQxBmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can fill in this code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_6sKh1exKdB",
        "colab_type": "text"
      },
      "source": [
        "### Tackling the Short Term Memory Problem\n",
        "As you know the LSTM cell is just an RNN cell that performs much better. Meaning that the training will converge much faster and it will detect long-term dependencies in the data.\n",
        "![image](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPSSAnoNDwU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.LSTM(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsUY4UIOGjQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Complete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnVR8AbLGjkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])\n",
        "\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid[:, 3::2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YpDr52nGkNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Complete"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXo8IOcXGEpn",
        "colab_type": "text"
      },
      "source": [
        "### Using 1D Convolutional Layers to Process Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca07Sam7DxV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\",\n",
        "                        input_shape=[None, 1]),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.GRU(20, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nf3csctGNPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmMuXwmoGOJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid[:, 3::2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae4dSiL7GSPP",
        "colab_type": "text"
      },
      "source": [
        "### Wavenet\n",
        "* Stacked 1D convolutional layer\n",
        "* Double dialation rate at every layer\n",
        "* Each Conv1d layer see half the inputs as the one under it\n",
        "* The original paper used 3 blocks of 10 conv1d layers with dilations of 1,2,4,8,16,...,256,512\n",
        "\n",
        "https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDHG39IFGQDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
        "for rate in (1, 2, 4, 8) * 2:\n",
        "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n",
        "                                  activation=\"relu\", dilation_rate=rate))\n",
        "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
        "history = model.fit(X_train, Y_train, epochs=20,\n",
        "                    validation_data=(X_valid, Y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6oRi8UOS8PO",
        "colab_type": "text"
      },
      "source": [
        "### Final word\n",
        "The last 2 models we created should give the best performance. They even allow us to generate voice, translations, even compose music... all we need is data and maybe some GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhd0S6CYIWPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}